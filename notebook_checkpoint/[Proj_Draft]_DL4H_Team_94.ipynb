{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Project Info\n",
        "\n",
        "Team:   94\n",
        "\n",
        "Member: Qinxi Wang\n",
        "\n",
        "Email:  qinxiw2@illinois.edu\n",
        "\n",
        "Github: https://github.com/QinxiW/DLH_Cervical_Cancer_Local_Explanations"
      ],
      "metadata": {
        "id": "V5h8LOma6JSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "###   Background of the problem\n",
        "\n",
        "Ayad et al discuss the importance of identifying and assessing factors that increase the risk of cervical cancer for early detection and treatment. They point out that the results from ML algorithms often are like blackbox decisions, and hard for the clinical practitioner to understand and decide if they want to follow through with the cervical cancer diagnosis in model prediction or not. This is a difficult problem because the lack of transparency not only hampers the interpretability of results, but also raises concerns about the reliability and safety of integrating machine learning into critical healthcare decision-making processe.\n",
        "\n",
        "To address this, the authors examine various local explanation techniques aimed at elucidating a model's predictions for specific instances. The state of the art methods they used includes SHAP, TreeSHAP, LIME, DICE and other related interpretability methods.\n",
        "\n",
        "### Paper explanation\n",
        "They thus propose a framework to evaluate the quality of various explanations regarding cervical cancer risk, which involves computing different metrics to identify the most suitable explanation for assessing cervical cancer risk. In their experiments specifically, they provide empirical study analyzing the performances of different methods for explaining cervical cancer risk factors, then for each method they contextualize how various formulations of these explanations could be suitable for different patient scenarios and when they might not be appropriate, and provide recommendations to practitioners for utilizing different types of explanations in assessing and determining key factors affecting cervical cancer risk.\n",
        "\n",
        "The innovations lies in the proposed evaluation framework for the effectiveness of each explanation techniques specifically for predicting cervical cancer risk, evaluated using RemOve And Retrain (ROAR) metrics where a number of works deem stability, consistency, compactness and faithfulness as important facets of interpretability for healthcare domains overall. The paper offers a critical analysis of existing local interpretability methods for explaining cervical cancer risk factors, aiming to assist clinicians in choosing appropriate explanations for different patient contexts. The approach also have application contribution, as it enables selecting the suitable explanations to reason about cervical cancer diagnosis and can be extended to other healthcare applications and areas where explanation is critical, and paved the future work for a user study with clinicians to assess how features weighted sums may lead to context-specific explanations."
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "The paper draws the following conclusions from the experiments. We wonder if we can first reproduce, and then test if the hypotheses are valid:\n",
        "\n",
        "1.   All explainers agree on HPV being the most important risk factor for cervical cancer.\n",
        "2.   SHAP(SHapley Additive exPlanations) explainers have exactly the same top 10 most important features for Patient 1.\n",
        "3.   The most unstable explainers are those that depend on creating local neighborhoods\n",
        "4.   No single explanation performs optimally across patients and metrics\n",
        "5.   The top 30% important features given by TreeSHAP have the most impact on model learning.\n",
        "6.   LIME(Local Interpretable Model-Agnostic Explanations) is the most stable, most robust to removal of features, and SHAP the most consistent in terms of feature and rank agreements.\n",
        "\n",
        "We hope to test these hypotheses by repeat the process the authors have done in the paper, where we will first generate and compare the quality of each of the local explanation techniques, we will then examine the top features produced by each of the techniques, and assess the decrease in accuracy of models when successively removing a fraction of the top features for each explanation and see if the above hold. The paper came with various results, which we will also cover in the later section; for illustration purpose, the scope of reproducibility will be focused on Features and Methods -"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "img_dir = '/content/drive/MyDrive/features.png'\n",
        "img_dir2 = '/content/drive/MyDrive/methods.png'\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt\n",
        "img = cv2.imread(img_dir)\n",
        "img2 = cv2.imread(img_dir2)\n",
        "\n",
        "cv2_imshow(img)\n",
        "cv2_imshow(img2)"
      ],
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This is the core part of the project draft. Here we will go over four subsections **data**, **model**, **training** and **evaluation** in our experiment and analysis work so far."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from typing import List\n",
        "from sklearn.preprocessing import RobustScaler,StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from plotly.offline import plot, iplot, init_notebook_mode\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "-fyPuT9MfEhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install psutil -U kaleido\n",
        "# import plotly.io as pio"
      ],
      "metadata": {
        "id": "05HpADMOfak_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "\n",
        "The dataset comes with 36 columns and 859 rows. Among them we have 858 patients and 35 features, including demographic information such as age and number of pregnancies, clinical tests such as Hinselmann and Citology, sexually transmitted diseases such as HPV and AIDS, and diagnosis taken by the patients such as HPV and CIN. All the data formats in the dataset are numbers or string, and all structured data, so from a data size perspective the memory and computation needed for working with is feasible.\n",
        "\n",
        "  * Source of the data:\n",
        "  \n",
        "  The dataset used in the paper is available from the UCI repository (Fernandes et al., 2017). It is open source, and also available on Kaggle which we could access and download directly at https://www.kaggle.com/datasets/loveall/cervical-cancer-risk-classification?resource=download\n",
        "  \n",
        "  * Statistics:\n",
        "  \n",
        "  Here are the high-level subtopics we will cover in the code snippets below.\n",
        "\n",
        "  1. ovreview of unique values of each column\n",
        "  2. data distribution analysis, in the order of: top features correlated with cancer, Pregnancy Distribution by Age, Correlation of diagnoses,\n",
        "  3. more in-depth data analysis, including number of sex partner by age buckets created, Proportions of women who have Cervical Cancer / HPV, Hormonal Contraceptives and Cervical Cancer.\n",
        "\n",
        "\n",
        "  * Data process:\n",
        "\n",
        "  Here are the high-level subtopics we will cover in the code snippets below.\n",
        "\n",
        "  1. convert all features to numeric column types if they are not already\n",
        "  2. bucket ages into 8 categories from \"Child\" to \"70+\"\n",
        "  3. aggregating various different std columns into a \"total_std\" new col.\n",
        "  4. dealing with imbalanced data\n",
        "  5. create train/test dataset for later training and evaluation\n",
        "\n",
        "  \n",
        "  * Illustration:\n",
        "  \n",
        "  We will be printing results, plotting figures for illustration for each of the sub-Statistics and Data processing in order below."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistics"
      ],
      "metadata": {
        "id": "RsoHYiJazDM-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# Data notes\n",
        "# dir and function to load raw data\n",
        "# raw_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "risk_factor_df = pd.read_csv('/content/drive/My Drive/kag_risk_factors_cervical_cancer.csv', delimiter=',', encoding='utf-8')\n",
        "risk_factor_df.head()\n",
        "\n",
        "# def load_raw_data(raw_data_dir):\n",
        "#   # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "#   return None\n",
        "\n",
        "# raw_data = load_raw_data(raw_data_dir)\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  return None\n",
        "\n",
        "# process raw data\n",
        "def process_data(raw_data):\n",
        "    # implement this function to process the data as you need\n",
        "  return None\n",
        "\n",
        "# processed_data = process_data(raw_data)\n",
        "\n",
        "''' you can load the processed data directly\n",
        "processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "def load_processed_data(raw_data_dir):\n",
        "  pass\n",
        "\n",
        "'''\n",
        "def print_unique_values_df(df: pd.DataFrame):\n",
        "    for col in list(df):\n",
        "        print(\"Unique Values for \"'{}'\":{}\".format(str(col), risk_factor_df[col].unique()))\n",
        "        print(\"dtype for {} is :{}\".format(str(col), risk_factor_df[col].dtypes))\n",
        "        print(\"-\" * 150)\n",
        "\n",
        "\n",
        "def print_unique_values_for_col(df: pd.DataFrame, col_names: List[str] = None):\n",
        "    for col in col_names:\n",
        "        print(\"Unique Values for \"'{}'\":{}\".format(str(col), risk_factor_df[col].unique()))\n",
        "\n",
        "print_unique_values_df(risk_factor_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code comment is used as inline annotations for your coding\n",
        "# risk_factor_df = pd.read_csv('/content/drive/My Drive/kag_risk_factors_cervical_cancer.csv', delimiter=',', encoding='utf-8')\n",
        "risk_factor_df.head()"
      ],
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 840 non HPV, 18 HPV\n",
        "\n",
        "risk_factor_df[risk_factor_df['Dx:HPV'] == 1]\n",
        "risk_factor_df['Dx:HPV'].value_counts()"
      ],
      "metadata": {
        "id": "o5gDBWDHfq_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no nulls\n",
        "risk_factor_df.isna().sum()"
      ],
      "metadata": {
        "id": "_ZQQPBFOfwOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "risk_factor_df.info()"
      ],
      "metadata": {
        "id": "yQLMcMMwfzgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "risk_factor_df.describe()"
      ],
      "metadata": {
        "id": "Km3fNbrNzPdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Processing"
      ],
      "metadata": {
        "id": "2CpHfc-3y9Ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#these columns are not of type object, but are of type numeric\n",
        "cols_to_convert = ['Number of sexual partners', 'First sexual intercourse', 'Num of pregnancies', 'Smokes',\n",
        "                   'Smokes (years)', 'Smokes (packs/year)', 'Hormonal Contraceptives',\n",
        "                   'Hormonal Contraceptives (years)', 'IUD', 'IUD (years)', 'STDs', 'STDs (number)',\n",
        "                   'STDs:condylomatosis', 'STDs:cervical condylomatosis', 'STDs:vaginal condylomatosis',\n",
        "                   'STDs:vulvo-perineal condylomatosis', 'STDs:syphilis', 'STDs:pelvic inflammatory disease',\n",
        "                   'STDs:genital herpes', 'STDs:molluscum contagiosum', 'STDs:AIDS', 'STDs:HIV', 'STDs:Hepatitis B',\n",
        "                   'STDs:HPV', 'STDs: Time since first diagnosis',\n",
        "                   'STDs: Time since last diagnosis']\n",
        "# for i in range(0,len(cols_to_convert)):\n",
        "#     print(\"{}={}\".format(i,cols_to_convert[i]))\n",
        "risk_factor_df[cols_to_convert] = risk_factor_df[cols_to_convert].apply(pd.to_numeric, errors=\"coerce\")\n",
        "risk_factor_df[cols_to_convert].fillna(np.nan, inplace=True)\n",
        "imp = SimpleImputer(strategy=\"median\")\n",
        "X = imp.fit_transform(risk_factor_df)\n",
        "risk_factor_df = pd.DataFrame(X, columns=list(risk_factor_df.columns))\n"
      ],
      "metadata": {
        "id": "Q1Ty6B1wgHL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def age_cat(age):\n",
        "    if age < 12:\n",
        "        return \"Child\"\n",
        "    elif age < 20:\n",
        "        return \"Teen\"\n",
        "    elif age < 30:\n",
        "        return \"20's\"\n",
        "    elif age < 40:\n",
        "        return \"30's\"\n",
        "    elif age < 50:\n",
        "        return \"40's\"\n",
        "    elif age < 60:\n",
        "        return \"50's\"\n",
        "    elif age < 70:\n",
        "        return \"60's\"\n",
        "    else:\n",
        "        return \"70+\"\n",
        "\n",
        "\n",
        "risk_factor_df[\"Age\"] = risk_factor_df[\"Age\"].astype(int)\n",
        "risk_factor_df[\"age_cat\"] = risk_factor_df[\"Age\"].apply(age_cat)"
      ],
      "metadata": {
        "id": "_cuV5b6XgKt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_cols = {'STDs:condylomatosis',\n",
        "            'STDs:cervical condylomatosis',\n",
        "            'STDs:vaginal condylomatosis',\n",
        "            'STDs:vulvo-perineal condylomatosis',\n",
        "            'STDs:syphilis',\n",
        "            'STDs:pelvic inflammatory disease',\n",
        "            'STDs:genital herpes',\n",
        "            'STDs:molluscum contagiosum',\n",
        "            'STDs:AIDS',\n",
        "            'STDs:HIV',\n",
        "            'STDs:Hepatitis B',\n",
        "            'STDs:HPV'}\n",
        "\n",
        "risk_factor_df[\"total_std\"] = risk_factor_df[list(std_cols)].sum(axis=1)\n",
        "std_agg = risk_factor_df.groupby(\"age_cat\", as_index=False)[list(std_cols)].sum()\n",
        "\n",
        "test_cols = [\"Hinselmann\", \"Schiller\", \"Citology\", \"Biopsy\"]\n",
        "risk_factor_df[\"total_tests\"] = risk_factor_df[test_cols].sum(axis = 1)\n",
        "\n",
        "to_int_and_beyond = {\"total_tests\",\n",
        "                     \"total_std\",\n",
        "                     \"Smokes\",\n",
        "                     \"Biopsy\",\n",
        "                     \"Dx:Cancer\",\n",
        "                     \"Num of pregnancies\",\n",
        "                     \"Number of sexual partners\",\n",
        "                     \"First sexual intercourse\",\n",
        "                     \"Hormonal Contraceptives\",\n",
        "                     \"IUD\",\n",
        "                     \"STDs\",\n",
        "                     \"STDs (number)\",\n",
        "                     \"STDs: Number of diagnosis\",\n",
        "                     \"Dx:CIN\",\n",
        "                     \"Dx:HPV\",\n",
        "                     \"Dx\",\n",
        "                     \"Hinselmann\",\n",
        "                     \"Schiller\",\n",
        "                     \"Biopsy\",\n",
        "                     \"Citology\"}\n",
        "\n",
        "to_int_and_beyond = to_int_and_beyond.union(std_cols)\n",
        "\n",
        "for col in to_int_and_beyond:\n",
        "    risk_factor_df[col] = risk_factor_df[col].astype(int)\n",
        "\n",
        "# risk_factor_df.info()\n",
        "\n"
      ],
      "metadata": {
        "id": "KA-Kh9oXgNd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analysis"
      ],
      "metadata": {
        "id": "RLNQNzd80Alo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 7\n",
        "target = label = \"Dx:Cancer\"\n",
        "corr = risk_factor_df.select_dtypes(include=np.number).corr()\n",
        "\n",
        "x = corr.nlargest(n,target).index\n",
        "corr_df =  risk_factor_df[list(x)]\n",
        "corr = corr_df.corr()\n",
        "fig = px.imshow(corr,color_continuous_scale = \"PuBu\")\n",
        "fig.update_layout(title=\"Top \"+str(n)+\" Features Correlated With \"+str(target).capitalize())\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "jHokCALHgS_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stats(x):\n",
        "    temp1=(df[[x,label]].value_counts(normalize=True).round(decimals=3)*100).reset_index().rename(columns={0:'Overall_Percent'})\n",
        "    Coloumn_To_Aggregate=[x,label]\n",
        "    df6=pd.merge(df.groupby(Coloumn_To_Aggregate).size().reset_index(name='ind_siz'),\n",
        "                 df.groupby(Coloumn_To_Aggregate[:-1]).size().reset_index(name='Total'), on =Coloumn_To_Aggregate[:-1])\n",
        "    df6['Category_Percent']=round((df6['ind_siz']/df6['Total'])*100 ,2)\n",
        "    temp2=df6[[x,label,'Category_Percent']]\n",
        "    temp3=temp1.merge(temp2,on=[x,label])\n",
        "    return temp3.pivot(columns=x,index=label)\n",
        "\n",
        "df=risk_factor_df\n",
        "label='age_cat'\n",
        "\n",
        "stats('Dx:Cancer')\n"
      ],
      "metadata": {
        "id": "m5GIfuvpgXpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_dist = px.histogram(risk_factor_df, x=\"Age\", marginal=\"box\", color_discrete_sequence=[\"palevioletred\"])\n",
        "age_dist.update_layout(title=\"Age distribution\")\n",
        "age_dist.show()"
      ],
      "metadata": {
        "id": "46ffRlQzgasG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_preg_bar = px.box(risk_factor_df.sort_values(by=\"Age\",ascending=True), x=\"age_cat\", y=\"Num of pregnancies\",\n",
        "                      color_discrete_sequence=[\"darkblue\"], points=\"outliers\",\n",
        "                      category_orders=[\"Teenager\", \"Twenties\", \"Thirties\", \"Forties\", \"Fifties\",\"Sixties\",\n",
        "                                       \"Seventy and over\"])\n",
        "age_preg_bar.update_xaxes(title=\"Age Category\")\n",
        "age_preg_bar.update_yaxes(title=\"Number of Pregnancies\")\n",
        "age_preg_bar.update_layout(title=\"Distribution of number of pregnancies per age group\")\n",
        "age_preg_bar.show()\n"
      ],
      "metadata": {
        "id": "gsbNaYIv0FmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_num_sex_partners = px.box(risk_factor_df.sort_values(by=\"Age\",ascending=True), x=\"age_cat\", y=\"Number of sexual partners\",\n",
        "                      color_discrete_sequence=[\"blue\"], points=\"outliers\",\n",
        "                      category_orders=[\"Teenager\", \"Twenties\", \"Thirties\", \"Forties\", \"Fifties\",\n",
        "                                       \"Seventy and over\"])\n",
        "age_num_sex_partners.update_xaxes(title=\"Age Category\")\n",
        "age_num_sex_partners.update_yaxes(title=\"Number of Sexual Partners\")\n",
        "age_num_sex_partners.update_layout(title=\"Distribution of number of sexual partners per age group\")\n",
        "age_num_sex_partners.show()\n"
      ],
      "metadata": {
        "id": "iKnJIxHy0OZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_num_sex_partners = px.scatter(risk_factor_df, x=\"Age\",\n",
        "                                  y=\"Number of sexual partners\",\n",
        "                                  trendline=\"ols\",\n",
        "                                  opacity=0.4,\n",
        "                                  color=\"Num of pregnancies\",\n",
        "                                  color_continuous_scale=\"rdbu\",)\n",
        "age_num_sex_partners.update_layout(title=\"Age vs Number of Sexual Partners\")\n",
        "age_num_sex_partners.show()\n"
      ],
      "metadata": {
        "id": "bvzpiJHA0Qwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# risk_factor_df.columns\n",
        "# exclude_columns = ['age_cat', 'total_std']\n",
        "\n",
        "# # Select columns excluding those in exclude_columns\n",
        "# selected_columns = risk_factor_df.columns[~risk_factor_df.columns.isin(exclude_columns)]"
      ],
      "metadata": {
        "id": "dp5F9XnM0fXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = 'Number of sexual partners'\n",
        "diagnoses_cols = [label,\n",
        "                  'Dx:CIN',\n",
        "                  'Dx:HPV']\n",
        "diagnoses_corr_matrix = risk_factor_df[diagnoses_cols].corr()\n",
        "# print(diagnoses_corr_matrix)\n",
        "diagnoses_heatmap = px.imshow(diagnoses_corr_matrix, aspect=\"auto\", color_continuous_scale=\"tealgrn\", text_auto=True)\n",
        "diagnoses_heatmap.show()\n"
      ],
      "metadata": {
        "id": "mw_N_FU60TmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.histogram(std_agg, x=\"age_cat\", y=list(std_cols), barmode=\"group\", histfunc=\"sum\")\n",
        "fig.update_layout(title=\"Sum of STD occurence across age categories\")\n",
        "fig.update_xaxes(title=\"Age Category\")\n",
        "fig.update_yaxes(title=\"Sum\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "fcvPo_x-2VWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_num_sex_partners = px.box(risk_factor_df.sort_values(by=\"Age\",ascending=True), x=\"age_cat\", y=\"total_std\",\n",
        "                      color_discrete_sequence=[\"blue\"], points=\"outliers\",\n",
        "                      category_orders=[\"Teenager\", \"Twenties\", \"Thirties\", \"Forties\", \"Fifties\",\n",
        "                                       \"Seventy and over\"])\n",
        "age_num_sex_partners.update_xaxes(title=\"Age Category\")\n",
        "age_num_sex_partners.update_yaxes(title=\"Number of Sexual Partners\")\n",
        "age_num_sex_partners.update_layout(title=\"Distribution of number of sexual partners per age group\")\n",
        "age_num_sex_partners.show()\n"
      ],
      "metadata": {
        "id": "suzmPOno2ZPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.histogram(risk_factor_df.query(\"total_std>=0\").sort_values(by=[\"total_std\", label], ascending=True),\n",
        "                   x=\"age_cat\",\n",
        "                   facet_col=\"total_std\",\n",
        "                   facet_row=label,\n",
        "                   color_discrete_sequence=[\"rebeccapurple\"],\n",
        "                   opacity=0.7)\n",
        "fig.update_layout(title=\"Count of women across age groups who have had one or more std\")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "bJ6zS1gY2ddZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.histogram(risk_factor_df.query(\"total_std>=0\").sort_values(by=[\"total_std\",\"Dx:HPV\"], ascending=True),\n",
        "                   x=\"age_cat\",\n",
        "                   facet_col=\"total_std\",\n",
        "                   facet_row=\"Dx:HPV\",\n",
        "                   color_discrete_sequence=[\"dodgerblue\"],\n",
        "                   opacity=0.7)\n",
        "fig.update_layout(title=\"Count of women across age groups who have had one or more std\")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "UHNdj_Tk2eMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.histogram(risk_factor_df.query(\"total_tests>0\").sort_values(by=\"total_tests\", ascending=True),\n",
        "                   x=\"age_cat\",\n",
        "                   facet_col=\"total_tests\",\n",
        "                   facet_row=label,\n",
        "                   color_discrete_sequence=[\"blueviolet\"],\n",
        "                   opacity=0.8)\n",
        "fig.update_layout(title=\"Count of women across age groups who have had one or more test\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "PH00_IWc2gi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.histogram(risk_factor_df.query(\"total_tests>0\").sort_values(by=[\"total_tests\",\"Dx:HPV\"], ascending=True),\n",
        "                   x=\"age_cat\",\n",
        "                   facet_col=\"total_tests\",\n",
        "                   facet_row=\"Dx:HPV\",\n",
        "                   color_discrete_sequence=[\"coral\"],\n",
        "                   opacity=0.8)\n",
        "fig.update_layout(title=\"Count of women across age groups who have had one or more test\")\n",
        "\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XaxjNH_J2jV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.ecdf(risk_factor_df, x=[\"Smokes (years)\",\n",
        "                                 \"Hormonal Contraceptives (years)\",\n",
        "                                 \"IUD (years)\"],\n",
        "              color_discrete_sequence=[\"crimson\", \"deepskyblue\", \"chartreuse\"])\n",
        "fig.update_xaxes(title=\"Years\")\n",
        "fig.update_layout(title=\"ECDF Plot\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "gZ_03UTu2luS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_category_range = {\n",
        "    \"Age<12\": \"Child\",\n",
        "    \"Age>=12 & Age<20\": \"Teen\",\n",
        "    \"Age>=20 & Age<30\": \"20's\",\n",
        "    \"Age>=30 & Age<40\": \"30's\",\n",
        "    \"Age>=40 & Age<50\": \"40's\",\n",
        "    \"Age>=50 & Age<60\": \"50's\",\n",
        "    \"Age>=60 & Age<70\": \"60's\",\n",
        "    \"Age>=70\": \"70+\"}\n",
        "age_prop_dict = {}\n",
        "col = \"Age\"  # Just to get the count\n",
        "for age_range, category in age_category_range.items():\n",
        "    age_prop_dict[category] = risk_factor_df.query(age_range)[col].count() / len(risk_factor_df)\n",
        "\n",
        "proportion_samples_df = pd.DataFrame.from_dict(age_prop_dict, orient=\"index\",\n",
        "                                               columns=[ \"Sample Proportion\"])\n",
        "proportion_samples_df = proportion_samples_df.reset_index()\n",
        "proportion_samples_df.columns = proportion_samples_df.columns.str.replace(\"index\",\"Category\")\n",
        "fig = px.pie(proportion_samples_df,\n",
        "             values='Sample Proportion',\n",
        "             names=\"Category\",\n",
        "             title='Age Category proportion of women sampled',color_discrete_sequence=px.colors.sequential.RdBu)\n",
        "fig.show()\n",
        "proportion_samples_df\n"
      ],
      "metadata": {
        "id": "l-Wi8hI92nt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'domain'}, {'type': 'domain'}]],\n",
        "                    subplot_titles=[\"Cancer\", \"HPV\"])\n",
        "fig.add_trace(go.Pie(labels=risk_factor_df[\"age_cat\"],\n",
        "                     values=risk_factor_df[label],\n",
        "                     name=\"Cancer\", marker_colors=px.colors.sequential.RdBu),\n",
        "              1, 1)\n",
        "fig.add_trace(go.Pie(labels=risk_factor_df[\"age_cat\"],\n",
        "                     values=risk_factor_df[\"Dx:HPV\"],\n",
        "                     name=\"HPV\", marker_colors=px.colors.sequential.RdBu),\n",
        "              1, 2)\n",
        "\n",
        "fig.update_traces(hole=.0, hoverinfo=\"label+percent+name\")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Proportion of women across age categories with a diagnosis of Cancer, HPV\",\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "YR7eeyvv2rLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hormonal_compariosn = risk_factor_df.groupby([\"age_cat\"], as_index=False)[[\"IUD\", \"Hormonal Contraceptives\"]].sum()\n",
        "fig = px.histogram(df_hormonal_compariosn, x=\"age_cat\", y=[\"IUD\", \"Hormonal Contraceptives\"], barmode=\"group\"\n",
        "                   , color_discrete_sequence=[\"darkcyan\", \"mediumorchid\"])\n",
        "\n",
        "fig.update_xaxes(title=\"Age Category\")\n",
        "fig.update_yaxes(title=\"Count\")\n",
        "fig.update_layout(title=\"Age Ranges of women who use Contraceptives\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "alJH7gET2t-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hormonal_contraceptives = risk_factor_df[\n",
        "    (risk_factor_df[\"Hormonal Contraceptives\"] == 1) & (risk_factor_df[\"IUD\"] == 0)]\n",
        "df_hormonal_contraceptives = df_hormonal_contraceptives.sort_values(by=[\"Smokes\", label])\n",
        "fig = px.histogram(df_hormonal_contraceptives, x=\"age_cat\", color=\"Smokes\", barmode=\"group\", facet_col=label,\n",
        "                   color_discrete_sequence=[\"darkcyan\", \"crimson\"])\n",
        "fig.update_xaxes(title=\"Age Category\")\n",
        "fig.update_yaxes(title=\"Count\")\n",
        "fig.update_layout(title=\"Age Ranges of women who use Hormonal Contraceptives\")\n",
        "# fig.for_each_annotation(lambda a: a.update(text=a.text.split(\":\")[-1]))\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "9PIpUJ_Q2wjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df_IUD_contraceptives = risk_factor_df[(risk_factor_df[\"Hormonal Contraceptives\"] == 0) & (risk_factor_df[\"IUD\"] == 1)]\n",
        "df_IUD_contraceptives = df_IUD_contraceptives.sort_values(by=[\"Smokes\", label], ascending=True)\n",
        "fig = px.histogram(df_IUD_contraceptives, x=\"age_cat\", color=\"Smokes\", barmode=\"group\", facet_col=label,\n",
        "                   color_discrete_sequence=[\"darkcyan\", \"crimson\"])\n",
        "fig.update_xaxes(title=\"Age Category\")\n",
        "fig.update_yaxes(title=\"Sum of IUD Usage across age category\")\n",
        "fig.update_layout(title=\"Age Ranges of women who use IUD's\")\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "pIWCkuXA20m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_both_contraceptives = risk_factor_df[(risk_factor_df[\"Hormonal Contraceptives\"] == 1) & (risk_factor_df[\"IUD\"] == 1)]\n",
        "df_both_contraceptives = df_both_contraceptives.sort_values(by=\"Smokes\")\n",
        "fig = px.histogram(df_both_contraceptives, x=\"age_cat\", color=\"Smokes\", barmode=\"group\", facet_col=label,\n",
        "                   color_discrete_sequence=[\"darkcyan\", \"crimson\"])\n",
        "fig.update_xaxes(title=\"Age Category\")\n",
        "fig.update_yaxes(title=\"Count\")\n",
        "fig.update_layout(title=\"Age Ranges of women who use BOTH Hormonal Contracepties and IUD's\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "zHnzTOOO22nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test / Train Split"
      ],
      "metadata": {
        "id": "RWtA-0q824vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test=risk_factor_df[['Number of sexual partners',\t'First sexual intercourse',\t'Num of pregnancies',\t'Smokes','Dx','Hormonal Contraceptives', 'total_std', 'total_tests', 'age_cat']].groupby('age_cat').mean()\n",
        "\n",
        "with open('summary.tex','w') as tf:\n",
        "    tf.write(test.round(2).to_latex())\n",
        "\n",
        "risk_factor_df.columns\n",
        "\n"
      ],
      "metadata": {
        "id": "p4NJpNqO28v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label=\"Dx:Cancer\"\n",
        "\n",
        "dx_cancer = px.histogram(risk_factor_df, y=label)\n",
        "dx_cancer.update_layout(bargap=0.2)\n",
        "dx_cancer.update_layout(title = \"Imbalanced Classes\")\n",
        "dx_cancer.show()\n",
        "\n",
        "X = risk_factor_df.drop([label, \"age_cat\"], axis=1)\n",
        "y = risk_factor_df[label].copy()\n",
        "\n"
      ],
      "metadata": {
        "id": "aaTmzHiz2_gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adasyn = ADASYN(random_state=42)\n",
        "x_adasyn,y_adasyn = adasyn.fit_resample(X,y)\n",
        "risk_factor_df = x_adasyn.join(y_adasyn)\n",
        "\n",
        "# ros = RandomOverSampler(random_state=42)\n",
        "# x_ros, y_ros = ros.fit_resample(X, y)\n",
        "# risk_factor_df = x_ros.join(y_ros)\n",
        "\n",
        "risk_factor_df[\"age_cat\"] = risk_factor_df[\"Age\"].apply(age_cat)\n",
        "\n",
        "dx_cancer = px.histogram(risk_factor_df, y=label)\n",
        "dx_cancer.update_layout(bargap=0.2)\n",
        "dx_cancer.update_layout(title = \"Balanced Classes\")\n",
        "dx_cancer.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "IUWeSEQS3Com"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rain_set = None\n",
        "test_set = None\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_idx, test_idx in split.split(risk_factor_df, risk_factor_df[\"age_cat\"]):\n",
        "    train_set = risk_factor_df.loc[train_idx]\n",
        "    test_set = risk_factor_df.loc[test_idx]\n",
        "cols_to_drop = [\"age_cat\",\"total_std\",\"total_tests\"]\n",
        "for set_ in (train_set, test_set):\n",
        "    for col in cols_to_drop:\n",
        "        set_.drop(col, axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "227GKlxW3GnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_set.drop(label, axis=1)\n",
        "y_train = train_set[label].copy()\n",
        "\n",
        "X_test = test_set.drop(label, axis=1)\n",
        "y_test = test_set[label].copy()\n",
        "\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "len(X_test.columns)"
      ],
      "metadata": {
        "id": "Ppw2qaOx3JTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## saving the data into csv for reuse\n",
        "# Without random var\n",
        "\n",
        "X_test.to_csv('/content/drive/My Drive/X_test.csv')\n",
        "y_test.to_csv('/content/drive/My Drive/y_test.csv')\n",
        "X_train.to_csv('/content/drive/My Drive/X_train.csv')\n",
        "y_train.to_csv('/content/drive/My Drive/y_train.csv')\n",
        "\n",
        "# With random var\n",
        "# Binary\n",
        "\n",
        "X_test.to_csv('/content/drive/My Drive/RX_test2.csv')\n",
        "y_test.to_csv('/content/drive/My Drive/Ry_test2.csv')\n",
        "X_train.to_csv('/content/drive/My Drive/RX_train2.csv')\n",
        "y_train.to_csv('/content/drive/My Drive/Ry_train2.csv')\n",
        "\n",
        "# Continuous\n",
        "\n",
        "X_test.to_csv('/content/drive/My Drive/RX_test.csv')\n",
        "y_test.to_csv('/content/drive/My Drive/Ry_test.csv')\n",
        "X_train.to_csv('/content/drive/My Drive/RX_train.csv')\n",
        "y_train.to_csv('/content/drive/My Drive/Ry_train.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "wxewwfa-3YLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "\n",
        "\n",
        "Ayad et al used 5 different model architectures that have widely been used in prior literature for cervical cancer risk assessment - namely Logistic Regression (LR), Random Forest (RF), Support Vector Machine (SVM), k-Nearest Neighbors (KNN), and Multilayer Perceptron (MLP).\n",
        "\n",
        "\n",
        "These models often show high variance and lack interpretability, so Ayad et al generated local explanations for each of them, and compared the explanations generated with the set of bench evaluation metrics, and summarized the approach for assessing the quality of different explanations with their algorithm for assessing local feature contribution.\n",
        "\n",
        "\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class my_model():\n",
        "#   # use this class to define your model\n",
        "#   pass\n",
        "\n",
        "# model = my_model()\n",
        "# loss_func = None\n",
        "# optimizer = None\n",
        "\n",
        "# def train_model_one_iter(model, loss_func, optimizer):\n",
        "#   pass\n",
        "\n",
        "# num_epoch = 10\n",
        "# # model training loop: it is better to print the training/validation losses during the training\n",
        "# for i in range(num_epoch):\n",
        "#   train_model_one_iter(model, loss_func, optimizer)\n",
        "#   train_loss, valid_loss = None, None\n",
        "#   print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression"
      ],
      "metadata": {
        "id": "GYEYOBtSNGmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': np.logspace(-5, 8, 15)}\n",
        "logreg = LogisticRegression()\n",
        "logreg_cv = GridSearchCV(logreg, param_grid, cv=10,refit=True).fit(X_train,y_train)\n",
        "logreg_cv = LogisticRegression(**logreg_cv.best_params_)"
      ],
      "metadata": {
        "id": "BazlZd69M6xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### random forrest"
      ],
      "metadata": {
        "id": "E3nwEGZvM7wZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_clf = RandomForestClassifier()"
      ],
      "metadata": {
        "id": "CB8qEUdovLa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN"
      ],
      "metadata": {
        "id": "VNBrSBwyNhb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_clf = KNeighborsClassifier()\n",
        "knn_param_grid = {\"n_neighbors\": list(np.arange(1, 100, 2))}\n",
        "knn_clf_cv = GridSearchCV(knn_clf, knn_param_grid, cv=10,refit=True).fit(X_train,y_train)\n",
        "knn_clf_cv = KNeighborsClassifier(**knn_clf_cv.best_params_)"
      ],
      "metadata": {
        "id": "IEi87NAKNj9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVC"
      ],
      "metadata": {
        "id": "RclYTdVGNmgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf = SVC()\n",
        "svc_param_grid = {'C': np.logspace(-3, 2, 6), 'gamma': np.logspace(-3, 2, 6), }\n",
        "svm_clf_cv = GridSearchCV(svm_clf, svc_param_grid, cv=5)"
      ],
      "metadata": {
        "id": "71vUCrEONn23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP"
      ],
      "metadata": {
        "id": "1uX_7ZTfNqCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "nn_clf = MLPClassifier()"
      ],
      "metadata": {
        "id": "mT4NkCHm34A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Add a random variable\n",
        "# # Binary\n",
        "\n",
        "# from scipy.stats import bernoulli\n",
        "\n",
        "# risk_factor_df['VAR_b']=bernoulli.rvs(.5, size=risk_factor_df.shape[0])\n",
        "\n",
        "# # Continuous\n",
        "\n",
        "# risk_factor_df['VAR_c']=np.random.normal(loc=0, scale=1, size=risk_factor_df.shape[0])\n",
        "\n",
        "# risk_factor_df.columns\n",
        "\n"
      ],
      "metadata": {
        "id": "TVImauHV6fwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from scipy.stats import bernoulli\n",
        "\n",
        "# X_test['VARB']=bernoulli.rvs(.5, size=X_test.shape[0])\n",
        "# X_train['VARB']=bernoulli.rvs(.5, size=X_train.shape[0])\n",
        "# X_test['VARC']=bernoulli.rvs(.5, size=X_test.shape[0])\n",
        "# X_train['VARC']=bernoulli.rvs(.5, size=X_train.shape[0])\n"
      ],
      "metadata": {
        "id": "9Bp_EBQ46wVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for col in X_test.columns:\n",
        "#   X_test[col]+=np.random.normal(loc=0, scale=.1, size=X_test.shape[0])\n",
        "#   X_train[col]+=np.random.normal(loc=0, scale=.1, size=X_train.shape[0])\n"
      ],
      "metadata": {
        "id": "XrskH-bz60FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from scipy.stats import bernoulli\n",
        "\n",
        "#binary\n",
        "# X_test['VAR']=bernoulli.rvs(.5, size=X_test.shape[0])\n",
        "# X_train['VAR']=bernoulli.rvs(.5, size=X_train.shape[0])\n",
        "\n",
        "\n",
        "\n",
        "# continous\n",
        "# X_test['VAR']=np.random.normal(loc=0, scale=1, size=X_test.shape[0])\n",
        "# X_train['VAR']=np.random.normal(loc=0, scale=1, size=X_train.shape[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "TxTBME5T62ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "This section we fit the five models mentioned in the Model section above. From the model architecture perspective, all the models that the paper uses are all regular supervised models and can reasonably train and eval within hours on a laptop computation power, so also feasible on that front. So the dataset is workable for our applications without needing to downsampling.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nn_clf.fit(X_train, y_train)\n",
        "\n",
        "col_names = [\"Classifier Name\", \"Accuracy Score\", \"Precision Score\",\n",
        "             \"Recall Score\", \"F1 Score\", \"AUROC\"]\n",
        "summary_df = pd.DataFrame(columns=col_names)\n",
        "\n",
        "est_name = []\n",
        "est_acc = []\n",
        "precision_score = []\n",
        "recall_score = []\n",
        "f1score = []\n",
        "est_conf_matrix = []\n",
        "roc=[]\n",
        "\n",
        "estimators = [\n",
        "    (\"LogisticRegression\", logreg_cv),\n",
        "    (\"RandomForestClassifier \", rnd_clf),\n",
        "    (\"KNeighborsClassifier\", knn_clf_cv),\n",
        "    (\"SupportVectorClassifier\", svm_clf_cv),\n",
        "    (\"MLPClassifier\", nn_clf)]\n"
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(estimators)):\n",
        "    clf_name = estimators[i][0]\n",
        "    clf = estimators[i][1]\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    #print(pd.crosstab(y_test,y_pred,rownames=[\"Actual\"],colnames=[\"predicted\"],margins=True))\n",
        "    roc.append(roc_auc_score(y_test, y_pred, average=None))\n",
        "    print('roc',roc)\n",
        "    est_name.append(estimators[i][0])\n",
        "    est_acc.append(accuracy_score(y_test, y_pred))\n",
        "    scores = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
        "    print('scores de '+str(clf_name), scores)\n",
        "    precision_score.append(scores[0])\n",
        "    recall_score.append(scores[1])\n",
        "    f1score.append(scores[2])\n",
        "    est_conf_matrix.append(confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "id": "qrybzWgCN5CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df[col_names[0]] = est_name\n",
        "summary_df[col_names[1]] = est_acc\n",
        "summary_df[col_names[2]] = precision_score\n",
        "summary_df[col_names[3]] = recall_score\n",
        "summary_df[col_names[4]] = f1score\n",
        "summary_df[col_names[5]] = roc\n",
        "\n"
      ],
      "metadata": {
        "id": "b5eBSymNN50F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimators"
      ],
      "metadata": {
        "id": "NLmqo2BTOEhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "For evaluation, we will go over the base suites of metrics, for each model:\n",
        "1. confusion matrix\n",
        "    - the counts of true positives, true negatives, false positives, and false negatives.\n",
        "2. accuracy\n",
        "    - how many total predictions did the model get right\n",
        "3. precision\n",
        "    - true positives rate by mode out of all positive predictions\n",
        "4. recall\n",
        "    - true positives that were correctly identified by the model out of all actual positives.\n",
        "5. F1\n",
        "    - harmonic mean of precision and recall\n",
        "6. AUC\n",
        "    - the area under the Receiver Operating Characteristic (ROC) curve, which measures the model's ability to discriminate between positive and negative classes across different threshold values.\n",
        "\n",
        "Later in the section we compare each model with others in visualization of the metrics aboves\n",
        "\n",
        "We also plan to use the standard precision, recall, and RemOve And Retrain (ROAR) (Hooker et al., 2018) mentioned in the paper for the faithfulness metric, where we will iteratively remove a subset of features from a dataset, and retrain the model on the reduced dataset to measure the changes in model accuracy or feature importance in each iteration, which will come later in the Results Analysis subsection.\n",
        "\n"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "color_scales = [\"agsunset\",\"teal\",\"purp\",\"viridis\",\"viridis\"]\n",
        "for i in range(0,len(est_conf_matrix)):\n",
        "    heatmap = px.imshow(est_conf_matrix[i],aspect=\"auto\",\n",
        "                        text_auto=True,\n",
        "                        color_continuous_scale=color_scales[i])\n",
        "    heatmap.update_layout(title = est_name[i])\n",
        "    heatmap.update_xaxes(title=\"Predicted\")\n",
        "    heatmap.update_yaxes(title=\"Actual\")\n",
        "    heatmap.show()\n"
      ],
      "metadata": {
        "id": "uXI5FSDpOKV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df"
      ],
      "metadata": {
        "id": "XzexwWcvOPl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# px.colors.sequential.RdBu\n",
        "#https://plotly.com/python/error-bars/\n",
        "#https://problemsolvingwithpython.com/06-Plotting-with-Matplotlib/06.07-Error-Bars/\n",
        "acc_comparison = px.bar(summary_df, x=\"Classifier Name\",\n",
        "                        y=col_names[1:len(col_names)], labels={\"value\":\"Test Accuracy\", \"variable\":\"Metrics\"}, text_auto=True,\n",
        "                        color_discrete_sequence=[\"deeppink\",\n",
        "                                                 \"deepskyblue\",\n",
        "                                                 \"darkviolet\",\n",
        "                                                 \"darkorange\",\n",
        "                                                 \"darkred\"],\n",
        "                        barmode=\"group\"\n",
        "                        #,error_y=[dict(type='data', array=[0.5, 1, 2],visible=True), dict(type='data', array=[0.5, 1, 2]), dict(type='data', array=[0.5, 1, 2],visible=True), dict(type='data', array=[0.5,1]), dict(type='data', array=[0.5, 1, 2, 2, 1])]\n",
        "                        #,error_y_minus = [dict(type='data', array=[0.5, 1, 2, 2, 1],visible=True), dict(type='data', array=[0.5, 1, 2]), dict(type='data', array=[2, 1]), dict(type='data', array=[0.5,1]), dict(type='data', array=[0.5, 1, 2, 2, 1])]\n",
        "                        )\n",
        "acc_comparison.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n",
        "'paper_bgcolor': 'rgba(0, 0, 0, 0)'\n",
        "})\n",
        "acc_comparison.show()\n",
        "\n",
        "# acc_comparison.write_image('/content/drive/My Drive/modelsperf.png')\n"
      ],
      "metadata": {
        "id": "xQMXadvhOV78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "Now that we finished training the model for Cervical Cancer risk factor prediction, we want to look into how locally explanables they each are to aid the healthcare process of diagnosis.\n",
        "\n",
        "\n",
        "The models are evaluated to be pretty great, as noted right above, they score perfect in accuracy, and high in precision, recall, f1, and AUORC. Using MLP model as an example, the model achieved 0.997024 in accuracy, 0.997042 for precision, 0.997024 for recall, 0.997143 for AUC. Now how reliable are these models really in serving real-world cancer risk factor prediction?\n",
        "\n",
        "In this section we will go over the Local Explanability results and analyses, and briefly go over our plan for the rest of the project time that we will continue to work on post this draft submission.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explainability results"
      ],
      "metadata": {
        "id": "j3W8JjJf4U4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Local methods\n",
        "\n",
        "# Generate local FI\n",
        "\n",
        "# !pip install shap\n",
        "# !pip install lime\n",
        "# !pip install interpret-community\n",
        "# !pip install alibi\n",
        "# !pip install treeinterpreter\n",
        "# !pip install SALib\n",
        "# !pip install dice-ml\n",
        "# !pip install pip install spectralcluster\n",
        "# !pip install -U kaleido"
      ],
      "metadata": {
        "id": "uqIazheIvtMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import PartialDependenceDisplay, partial_dependence\n",
        "from interpret_community.mimic.mimic_explainer import MimicExplainer\n",
        "from interpret_community.mimic.models import LinearExplainableModel\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from interpret.blackbox import MorrisSensitivity\n",
        "import shap\n",
        "import lime\n",
        "from lime import lime_tabular\n",
        "from treeinterpreter import treeinterpreter as ti\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import arange\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "auMMX4Tf7c0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GloSur=kernelSHAP=treeSHAP=samplingSHAP=limecontrib=ticontrib=dicecontrib=pd.DataFrame([[0.0]*X_test.shape[1]]*X_test.shape[0], columns=X_test.columns)\n",
        "fi_1=fi_2=fi_3=fi_4=fi_5=fi_6=fi_7={f'{x}':0.0 for x in X_test.columns}\n",
        "\n",
        "model = nn_clf\n",
        "\n",
        "res = dict()\n",
        "features=X_test.columns"
      ],
      "metadata": {
        "id": "vHZcwstl7fKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exclude_columns = ['VARB', 'VARC', 'VAR']\n",
        "\n",
        "# Select columns excluding those in exclude_columns\n",
        "features = X_test.columns[~X_test.columns.isin(exclude_columns)]"
      ],
      "metadata": {
        "id": "J22iGKYf95_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-GLOSUR-\")\n",
        "# GloSur\n",
        "explainer = MimicExplainer(model,\n",
        "                          X_train[features],\n",
        "                          LinearExplainableModel,\n",
        "                          augment_data=False,\n",
        "                          features=features,\n",
        "                          model_task=\"classification\")\n",
        "global_explanation = explainer.explain_global(X_test[features])\n",
        "temp=pd.DataFrame(global_explanation.local_importance_values[1], columns=features)\n",
        "GloSur=GloSur.add(temp, fill_value=0)\n",
        "\n",
        "\n",
        "res = dict()\n",
        "res = global_explanation.get_feature_importance_dict()\n",
        "fi_1={k: fi_1.get(k, 0) + res.get(k, 0) for k in set(fi_1)}\n"
      ],
      "metadata": {
        "id": "vmUw-LuI7kDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"-KSHAP-\")\n",
        "# # KSHAP\n",
        "# explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
        "# shap_values = explainer.shap_values(X_test)\n",
        "# temp=pd.DataFrame(shap_values[1], columns=features)\n",
        "# kernelSHAP=kernelSHAP.add(temp, fill_value=0)\n",
        "\n",
        "# res = dict()\n",
        "# for i in list(kernelSHAP.columns):\n",
        "#   res[i]=np.mean(np.abs(kernelSHAP[i]))\n",
        "# fi_2={k: fi_2.get(k, 0) + res.get(k, 0) for k in set(fi_2)}\n",
        "# really slow to run"
      ],
      "metadata": {
        "id": "dBT98lkB-kBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"-TSHAP-\")\n",
        "# # TSHAP\n",
        "# explainer = shap.TreeExplainer(model,X_train)\n",
        "# shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# temp=pd.DataFrame(shap_values[1], columns=features)\n",
        "# treeSHAP=treeSHAP.add(temp, fill_value=0)\n",
        "\n",
        "# res = dict()\n",
        "# for i in list(treeSHAP.columns):\n",
        "#   res[i]=np.mean(np.abs(treeSHAP[i]))\n",
        "# fi_3={k: fi_3.get(k, 0) + res.get(k, 0) for k in set(fi_3)}\n"
      ],
      "metadata": {
        "id": "DwhKoLRBO11j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-SSHAP-\")\n",
        "# SSHAP\n",
        "explainer = shap.explainers.Sampling(model.predict_proba, X_train)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "temp=pd.DataFrame(shap_values[1], columns=features)\n",
        "samplingSHAP=samplingSHAP.add(temp, fill_value=0)\n",
        "\n",
        "res = dict()\n",
        "for i in list(samplingSHAP.columns):\n",
        "  res[i]=np.mean(np.abs(samplingSHAP[i]))\n",
        "fi_4={k: fi_4.get(k, 0) + res.get(k, 0) for k in set(fi_4)}"
      ],
      "metadata": {
        "id": "Z46CqnYjO3iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-LIME-\")\n",
        "\n",
        "# LIME\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,mode='classification',feature_names=X_test.columns)\n",
        "\n",
        "all=[]\n",
        "for i in range (len(X_test)):\n",
        "  exp = explainer.explain_instance(X_test.iloc[i], model.predict_proba, num_features=X_test.shape[1])\n",
        "  all.append(sorted(exp.as_map()[1]))\n",
        "\n",
        "\n",
        "all_res=[]\n",
        "for i in range(len(all)):\n",
        "  res = dict()\n",
        "  for j in range(len(all[0])):\n",
        "    res[features[j]] = all[i][j][1]\n",
        "  all_res.append(res)\n",
        "\n",
        "temp=pd.DataFrame(all_res, columns=features)\n",
        "limecontrib=limecontrib.add(temp, fill_value=0)\n",
        "\n",
        "res = dict()\n",
        "for j in list(limecontrib.columns):\n",
        "  res[j]=np.mean(np.abs(limecontrib[j]))\n",
        "fi_5={k: fi_5.get(k, 0) + res.get(k, 0) for k in set(fi_5)}\n"
      ],
      "metadata": {
        "id": "HZ9KS7jyO96G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-DICE-\")\n",
        "import dice_ml\n",
        "df=risk_factor_df\n",
        "d = dice_ml.Data(dataframe=df, continuous_features=list(X_test.columns), outcome_name=label)\n",
        "m = dice_ml.Model(model=model, backend=\"sklearn\")\n",
        "\n",
        "exp = dice_ml.Dice(d, m, method=\"random\")\n",
        "query_instance = X_test\n",
        "e1 = exp.generate_counterfactuals(query_instance, total_CFs=10, desired_range=None,\n",
        "                                  desired_class=\"opposite\",\n",
        "                                  permitted_range=None, features_to_vary=\"all\")\n",
        "\n",
        "imp = exp.local_feature_importance(query_instance, posthoc_sparsity_param=None)\n",
        "dicecontrib=pd.DataFrame.from_dict(imp.local_importance)\n",
        "\n",
        "res = dict()\n",
        "for j in list(dicecontrib.columns):\n",
        "  res[j]=np.mean(np.abs(dicecontrib[j]))\n",
        "fi_7={k: fi_7.get(k, 0) + res.get(k, 0) for k in set(fi_7)}"
      ],
      "metadata": {
        "id": "NI14h27ZPEZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "3IeEYGeLQOkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# GloSur.to_csv(\"/content/drive/My Drive/glosur.csv\", index=False)\n",
        "# kernelSHAP.to_csv(\"/content/drive/My Drive/Kshap.csv\", index=False)\n",
        "# #treeSHAP.to_csv(\"/content/drive/My Drive/Tshap.csv\", index=False)\n",
        "# samplingSHAP.to_csv(\"/content/drive/My Drive/Sshap.csv\", index=False)\n",
        "# limecontrib.to_csv(\"/content/drive/My Drive/lime.csv\", index=False)\n",
        "# #ticontrib.to_csv(\"/content/drive/My Drive/ti.csv\", index=False)\n",
        "# dicecontrib.to_csv(\"/content/drive/My Drive/dice.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "2yBae315TP7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyses\n",
        "\n",
        "Here we aggregate all the local explanations we generated for each of the model and explanation methods. Each produced a dataframe of feature importance of predicting the risk outcome of cervical cancer.\n",
        "The paper identified a set of most important features for assessing cervical cancer risk, we plan to rotate and systematically remove the top 30% most important features one by one and see if the model performance will have degradation, with the hope that it will help us understand the impact of specific features in determining cervical cancer, and analyze the five model's robustness or sensitivity to changes in the experiment.\n"
      ],
      "metadata": {
        "id": "rREwSt0oQdDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dics = []\n",
        "\n",
        "fi_1['Method'] = 'Surrogates'\n",
        "dics.append(fi_1)\n",
        "fi_2['Method'] = 'KSHAP'\n",
        "dics.append(fi_2)\n",
        "fi_3['Method'] = 'TSHAP'\n",
        "dics.append(fi_3)\n",
        "fi_4['Method'] = 'SSHAP'\n",
        "dics.append(fi_4)\n",
        "fi_5['Method'] = 'LIME'\n",
        "dics.append(fi_5)\n",
        "fi_6['Method'] = 'TI'\n",
        "dics.append(fi_6)\n",
        "fi_7['Method'] = 'DICE'\n",
        "dics.append(fi_7)\n",
        "\n",
        "\n",
        "dics = pd.DataFrame(dics)\n",
        "methods=dics['Method']\n",
        "dics['Method']=methods\n",
        "dics.to_csv(\"/content/drive/My Drive/dics_1.csv\", index=False)"
      ],
      "metadata": {
        "id": "CDRUaCeKSycP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dics"
      ],
      "metadata": {
        "id": "suCIbiDcaYvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_fi = dics\n",
        "all_fi.fillna(0, inplace=True)\n",
        "all_fi.iloc[:,:-1]=np.abs(all_fi.iloc[:,:-1])\n",
        "all_fi.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "methods=all_fi['Method'].to_list()\n",
        "weights=[GloSur, kernelSHAP, samplingSHAP, limecontrib, dicecontrib]"
      ],
      "metadata": {
        "id": "zxSjVtCAT3sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GloSur"
      ],
      "metadata": {
        "id": "FvykQ2sqZ6iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernelSHAP"
      ],
      "metadata": {
        "id": "o6JYJMU5Z8Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samplingSHAP"
      ],
      "metadata": {
        "id": "JHqYqfW8Z-ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "limecontrib"
      ],
      "metadata": {
        "id": "CosavEfbaI0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dicecontrib"
      ],
      "metadata": {
        "id": "LRIMVByAaBvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "methods # todo fix me\n",
        "weights=[GloSur, dicecontrib, dicecontrib, samplingSHAP, limecontrib, dicecontrib, dicecontrib]"
      ],
      "metadata": {
        "id": "6YDfrBIkaQcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instance=291\n",
        "var='W'\n",
        "maxx=10\n",
        "f=''\n",
        "\n",
        "one_instance=[]\n",
        "\n",
        "for i in range(len(methods)):\n",
        "  one_instance.append(weights[i].iloc[instance])\n",
        "\n",
        "one_instance=pd.DataFrame(one_instance, columns=X_test.columns)\n",
        "one_instance['methods']=methods\n",
        "one_instance.set_index('methods', inplace=True)\n",
        "\n",
        "one_instance.to_csv('/content/drive/My Drive/one_instance.csv')\n",
        "\n",
        "print('methods' in one_instance.columns)\n",
        "\n",
        "one_instance"
      ],
      "metadata": {
        "id": "rrh0abjNVQSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test)"
      ],
      "metadata": {
        "id": "vmWQD8GHV1PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## gscontrib\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,mode='classification',feature_names=X_test.columns)\n",
        "exp = explainer.explain_instance(X_test.iloc[instance], model.predict_proba, num_features=X_test.shape[1])\n",
        "\n",
        "items = gscontrib.iloc[instance].to_dict()\n",
        "t = []\n",
        "count=0\n",
        "for i, item in enumerate(items):\n",
        "  if abs(items[item]) > 0.0 :\n",
        "    t.append((i, items[item]))\n",
        "\n",
        "t = sorted(t, key=lambda tup: abs(tup[1]),  reverse=True)\n",
        "\n",
        "exp_test = {1: t[0:maxx]}\n",
        "\n",
        "\n",
        "exp.local_exp = exp_test\n",
        "exp.show_in_notebook(show_table=False,show_predicted_value=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "B_K7zm6xVv60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "fig = exp.as_pyplot_figure()\n",
        "plt.xlabel(\"Local Surrogates\")\n",
        "fig.savefig('/content/drive/My Drive/'+str(var)+'surrogate'+str(instance)+str(f)+'.png', bbox_inches='tight')\n"
      ],
      "metadata": {
        "id": "gwOEURIgV4ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## kercontrib\n",
        "items = kercontrib.iloc[instance].to_dict()\n",
        "t = []\n",
        "count=0\n",
        "for i, item in enumerate(items):\n",
        "  if abs(items[item]) > 0.0 :\n",
        "    t.append((i, items[item]))\n",
        "\n",
        "t = sorted(t, key=lambda tup: abs(tup[1]),  reverse=True)\n",
        "\n",
        "exp_test = {1: t[0:maxx]}\n",
        "\n",
        "\n",
        "exp.local_exp = exp_test\n",
        "exp.show_in_notebook(show_table=False,show_predicted_value=False)\n",
        "\n",
        "%matplotlib inline\n",
        "fig = exp.as_pyplot_figure()\n",
        "plt.xlabel(\"kernel SHAP\")\n",
        "fig.savefig('/content/drive/My Drive/'+str(var)+'kernelSHAP'+str(instance)+str(f)+'.png', bbox_inches='tight')\n",
        "\n"
      ],
      "metadata": {
        "id": "RE_VtxEgV8d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## trecontrib\n",
        "\n",
        "items = trecontrib.iloc[instance].to_dict()\n",
        "t = []\n",
        "count=0\n",
        "for i, item in enumerate(items):\n",
        "  if abs(items[item]) > 0.0 :\n",
        "    t.append((i, items[item]))\n",
        "\n",
        "t = sorted(t, key=lambda tup: abs(tup[1]),  reverse=True)\n",
        "\n",
        "exp_test = {1: t[0:maxx]}\n",
        "\n",
        "\n",
        "exp.local_exp = exp_test\n",
        "exp.show_in_notebook(show_table=True,show_predicted_value=False)\n",
        "\n",
        "%matplotlib inline\n",
        "fig = exp.as_pyplot_figure()\n",
        "plt.xlabel(\"Tree SHAP\")\n",
        "fig.savefig('/content/drive/My Drive/'+str(var)+'treeSHAP'+str(instance)+str(f)+'.png', bbox_inches='tight')\n",
        "\n"
      ],
      "metadata": {
        "id": "uZsbuMpdWEC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### samcontrib\n",
        "items = samcontrib.iloc[instance].to_dict()\n",
        "t = []\n",
        "count=0\n",
        "for i, item in enumerate(items):\n",
        "  if abs(items[item]) > 0.0 :\n",
        "    t.append((i, items[item]))\n",
        "\n",
        "t = sorted(t, key=lambda tup: abs(tup[1]),  reverse=True)\n",
        "\n",
        "exp_test = {1: t[0:maxx]}\n",
        "\n",
        "\n",
        "exp.local_exp = exp_test\n",
        "exp.show_in_notebook(show_table=True,show_predicted_value=False)\n",
        "\n",
        "%matplotlib inline\n",
        "fig = exp.as_pyplot_figure()\n",
        "plt.xlabel(\"Sampling SHAP\")\n",
        "fig.savefig('/content/drive/My Drive/'+str(var)+'samplingSHAP'+str(instance)+str(f)+'.png', bbox_inches='tight')\n",
        "\n"
      ],
      "metadata": {
        "id": "31udnHJHWMfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### lime\n",
        "\n",
        "items = limecontrib.iloc[instance].to_dict()\n",
        "t = []\n",
        "count=0\n",
        "for i, item in enumerate(items):\n",
        "  if abs(items[item]) > 0.0 :\n",
        "    t.append((i, items[item]))\n",
        "\n",
        "t = sorted(t, key=lambda tup: abs(tup[1]),  reverse=True)\n",
        "\n",
        "exp_test = {1: t[0:maxx]}\n",
        "\n",
        "\n",
        "exp.local_exp = exp_test\n",
        "exp.show_in_notebook(show_table=True,show_predicted_value=False)\n",
        "\n",
        "%matplotlib inline\n",
        "fig = exp.as_pyplot_figure()\n",
        "plt.xlabel(\"LIME\")\n",
        "fig.savefig('/content/drive/My Drive/'+str(var)+'lime'+str(instance)+str(f)+'.png', bbox_inches='tight')\n",
        "\n"
      ],
      "metadata": {
        "id": "Mhhms5S1WWQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ticontrib\n",
        "\n",
        "items = ticontrib.iloc[instance].to_dict()\n",
        "t = []\n",
        "count=0\n",
        "for i, item in enumerate(items):\n",
        "  if abs(items[item]) > 0.0 :\n",
        "    t.append((i, items[item]))\n",
        "\n",
        "t = sorted(t, key=lambda tup: abs(tup[1]),  reverse=True)\n",
        "\n",
        "exp_test = {1: t[0:maxx]}\n",
        "\n",
        "\n",
        "exp.local_exp = exp_test\n",
        "exp.show_in_notebook(show_table=False,show_predicted_value=False)\n",
        "\n",
        "%matplotlib inline\n",
        "fig = exp.as_pyplot_figure()\n",
        "plt.xlabel(\"Tree Interpreter\")\n",
        "fig.savefig('/content/drive/My Drive/'+str(var)+'ti'+str(instance)+str(f)+'.png', bbox_inches='tight')\n",
        "\n"
      ],
      "metadata": {
        "id": "qSHQE3e6Wa14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### dice\n",
        "\n",
        "items = dicecontrib.iloc[instance].to_dict()\n",
        "t = []\n",
        "count=0\n",
        "for i, item in enumerate(items):\n",
        "  if abs(items[item]) > 0.0 :\n",
        "    t.append((i, items[item]))\n",
        "\n",
        "t = sorted(t, key=lambda tup: abs(tup[1]),  reverse=True)\n",
        "\n",
        "exp_test = {1: t[0:maxx]}\n",
        "\n",
        "\n",
        "exp.local_exp = exp_test\n",
        "exp.show_in_notebook(show_table=False,show_predicted_value=False)\n",
        "\n",
        "%matplotlib inline\n",
        "fig = exp.as_pyplot_figure()\n",
        "plt.xlabel(\"DiCE\")\n",
        "fig.savefig('/content/drive/My Drive/'+str(var)+'dice'+str(instance)+str(f)+'.png', bbox_inches='tight')"
      ],
      "metadata": {
        "id": "wMa6NcUAWi6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ROAR\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "def roar(featImp, feature_to_predict, datapath, savepath, dataname):\n",
        "\n",
        "  a=['ro--', 'go--', 'mo--', 'yo--', 'co--', 'ko--', 'bo--']\n",
        "  pourc=[0,10,20,30,40,60,70,90]\n",
        "  font = {'size'   : 14}\n",
        "\n",
        "  plt.rc('font', **font)\n",
        "\n",
        "  df = datapath\n",
        "  X = df[list(df.columns.drop([feature_to_predict, 'age_cat']))]\n",
        "  y = df[feature_to_predict]\n",
        "\n",
        "  for k in range(featImp.shape[0]):\n",
        "    accuracies=[]\n",
        "    new=[]\n",
        "    for i in pourc:\n",
        "\n",
        "      fi= featImp.iloc[k,:]\n",
        "      fi.drop('Method', inplace=True)\n",
        "      fi=fi.to_dict()\n",
        "      fi=dict(sorted(fi.items(), key=lambda x:x[1], reverse=True))\n",
        "\n",
        "      fii=list(fi.keys())\n",
        "\n",
        "      # df= pd.read_csv(datapath)\n",
        "\n",
        "\n",
        "      top=int((len(df.columns)*i)/100)\n",
        "      fii = fii[top:]\n",
        "\n",
        "      #df.drop(fii[0:top], axis=1, inplace=True)\n",
        "      new=fii\n",
        "      new.append(feature_to_predict)\n",
        "      df = datapath\n",
        "      df=df[new]\n",
        "\n",
        "\n",
        "      model = RandomForestClassifier(random_state = 42)\n",
        "      # model.fit(X, y)\n",
        "      scores = cross_val_score(model, X, y, cv=10)\n",
        "      accuracies.append(np.mean(scores))\n",
        "\n",
        "    plt.plot(pourc, accuracies, a[k], label=featImp['Method'][k])\n",
        "\n",
        "\n",
        "  plt.xlabel('% removed features for Cervical cancer risk factors')\n",
        "\n",
        "\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.savefig(savepath+'roar.png', bbox_inches='tight', dpi=300)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "_CP2ZbPDWot4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# risk_factor_df.head()\n",
        "# risk_factor_df[label]\n",
        "# (risk_factor_df[list(risk_factor_df.columns.drop(label))])\n",
        "X=risk_factor_df[list(risk_factor_df.columns.drop([label, 'age_cat']))]\n",
        "y=np.array(risk_factor_df[label])\n",
        "\n",
        "model = RandomForestClassifier(random_state = 42)\n",
        "model.fit(X, y)\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=10)"
      ],
      "metadata": {
        "id": "1z7nOdw7blex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "hb4Wx-CDdnla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datapath= risk_factor_df #'/content/drive/My Drive/kag_risk_factors_cervical_cancer.csv'\n",
        "savepath= '/content/drive/My Drive/'\n",
        "dataname='Cervical cancer'\n",
        "\n",
        "roar(all_fi, label, datapath, savepath, dataname)"
      ],
      "metadata": {
        "id": "GX73tVeOWu89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "txhhaMHbXGDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plan\n",
        "\n",
        "Our plan for the remaining work include the following two main areas, post the draft work submission:\n",
        "\n",
        "1. Feature contributions plot, computing, measuring, and benchmarking Consistency, Compactness, and Stability of the local explanations generated with each method.\n",
        "\n",
        "2. Validate Feature and Rank disagreement shown in the paper, and see if we reproduce the same feature and rank agreement distribution among the key explanation methods like visualized in the original paper."
      ],
      "metadata": {
        "id": "FhPkVGXFQT4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "#### Main work:\n",
        "\n",
        "Ayad, C. W., Bonnier, T., Bosch, B., Read, J., & Parbhoo, S. (2023). Which Explanation Makes Sense? A Critical Evaluation of Local Explanations for Assessing Cervical Cancer Risk Factors.\n",
        "\n",
        "\n",
        "#### Local method reference:\n",
        "\n",
        "\n",
        "Kelwin Fernandes, Jaime S Cardoso, and Jessica Fernandes. Transfer learning with partial observability applied to cervical cancer screening. In Pattern Recognition and Image Analysis: 8th Iberian Conference, IbPRIA 2017, Faro, Portugal, June 20-23, 2017, Proceedings 8, pages 243–250. Springer, 2017.\n",
        "\n",
        "Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A Benchmark for\n",
        "Interpretability Methods in Deep Neural Networks, June 2018\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}